{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing: Winning Jeopardy\n",
    "\n",
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. I am going to work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help to win.\n",
    "\n",
    "The dataset is named jeopardy.csv and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which can be downloaded [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/).\n",
    "\n",
    "Each row in the dataset represents a single question on a single episode of Jeopardy. Here are explanations of each column:\n",
    "\n",
    "* Show Number -- the Jeopardy episode number of the show this question was in.\n",
    "* Air Date -- the date the episode aired.\n",
    "* Round -- the round of Jeopardy that the question was asked in. Jeopardy has several rounds as each episode progresses.\n",
    "* Category -- the category of the question.\n",
    "* Value -- the number of dollars answering the question correctly is worth.\n",
    "* Question -- the text of the question.\n",
    "* Answer -- the text of the answer.\n",
    "\n",
    "First I am going to read the dataset and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "jeopardy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the column names have spaces in front, I am going to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns = jeopardy.columns.str.strip()\n",
    "jeopardy.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a close look at the format of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Show Number  19999 non-null  int64 \n",
      " 1   Air Date     19999 non-null  object\n",
      " 2   Round        19999 non-null  object\n",
      " 3   Category     19999 non-null  object\n",
      " 4   Value        19999 non-null  object\n",
      " 5   Question     19999 non-null  object\n",
      " 6   Answer       19999 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "jeopardy.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the columns\n",
    "\n",
    "One messy aspect about the Jeopardy dataset is that it contains text. Text can contain punctuation and different capitalization, which will make it hard for us to compare the text of an answer to the text of a question. We would like to make this process easier for ourselves, so we’ll need to process the text data in this step. The process of cleaning text in data analysis is sometimes called normalization. More specifically, we want ensure that we lowercase all of the words and any remove punctuation. We remove punctuation because it ensures that the text stays as purely letters. Before normalization, the terms Don’t and don’t are considered to be different words, and we don’t want this. \n",
    "\n",
    "Before starting the analysis, we need to normalize and fix the datatypes of some columns. I need to lowercase `Question` and `Answer` columns and remove the punctuation. the `Value` column should be numeric and the `Air Date `should be a datetime.\n",
    "\n",
    "First I am going to write a function to get in a string and return that string in lowercase and without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def normalize(text):\n",
    "    text  = text.lower()\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# test normalize function\n",
    "normalize(\"Hello! How are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the normalize function to `Question` and `Answer` columns and save the result in `clean_question` and `clean_answer` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    for the last 8 years of his life galileo was u...\n",
       "1    no 2 1912 olympian football star at carlisle i...\n",
       "2    the city of yuma in this state has a record av...\n",
       "3    in 1963 live on the art linkletter show this c...\n",
       "4    signer of the dec of indep framer of the const...\n",
       "Name: clean_question, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normalize)\n",
    "jeopardy['clean_question'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    copernicus\n",
       "1    jim thorpe\n",
       "2       arizona\n",
       "3     mcdonalds\n",
       "4    john adams\n",
       "Name: clean_answer, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normalize)\n",
    "jeopardy['clean_answer'].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the `Value` column I am going to remove the dollar sign from the beginning, convert it from text to numeric and save the result to a new column called `clean_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_value(value):\n",
    "    value = re.sub('[^\\w\\s]', '', value)\n",
    "    try:\n",
    "        value_int = int(value)\n",
    "    except ValueError:\n",
    "        value_int = 0\n",
    "    return value_int\n",
    "# test\n",
    "normalize_value('$200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply normalize_value function to Value column\n",
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Air Date` column should also be datatime to enable us to work with easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the types of all columns especially the new ones again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Show Number     19999 non-null  int64         \n",
      " 1   Air Date        19999 non-null  datetime64[ns]\n",
      " 2   Round           19999 non-null  object        \n",
      " 3   Category        19999 non-null  object        \n",
      " 4   Value           19999 non-null  object        \n",
      " 5   Question        19999 non-null  object        \n",
      " 6   Answer          19999 non-null  object        \n",
      " 7   clean_question  19999 non-null  object        \n",
      " 8   clean_answer    19999 non-null  object        \n",
      " 9   clean_value     19999 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(7)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "jeopardy.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study\n",
    "\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "* How often the answer is used for a question.\n",
    "* How often new questions are repeats of older questions.\n",
    "\n",
    "To answer the second question I need to figure out how often complex words (> 6 characters) reoccur and for the first question I need to see how many times words in the answer also occur in the question.\n",
    "\n",
    "let's start with the first question. I am going to write a function to calculate for each question the ratio of the number of words in answers that are found in questions. Then I am going to apply it to all of the questions and calculate the average of them. In this function, 'the' is removed from the words that are investigated since in not a valuable word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05900196524977763"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_matches_ratio(row):\n",
    "    answer = row['clean_answer']\n",
    "    question = row['clean_question']\n",
    "    split_answer = answer.split()\n",
    "    split_question = question.split()\n",
    "    match_count = 0\n",
    "    if 'the' in split_answer:\n",
    "        split_answer.remove('the')\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    for item in split_answer:\n",
    "        if item in split_question:\n",
    "            match_count += 1\n",
    "    return match_count/len(split_answer)\n",
    "  \n",
    "# use apply() to loop over all the rows \n",
    "jeopardy['answer_in_question'] = jeopardy.apply(count_matches_ratio, axis = 1)  \n",
    "jeopardy['answer_in_question'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average about 6% of the words of answers are found in the questions. So the chance of deducing the answer from the question is quite low.\n",
    "\n",
    "## Repeated questions\n",
    "\n",
    "Let's go through the second question and investigate how often new questions are repeated of older ones. I can not completely answer this question the dataset includes only 10% of the full jeopardy question dataset but I am going to investigate it.\n",
    "\n",
    "I am going to check if the terms with six or more characters in questions have been used previously or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.689481997219586"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "# get unique set of words\n",
    "terms_used = set()\n",
    "# sorted date, then it is clear to see what is a new question\n",
    "jeopardy.sort_values('Air Date', inplace = True)\n",
    "# loop over data frame with index cout\n",
    "for i, row in jeopardy.iterrows():\n",
    "    # get list of the words in a question\n",
    "    split_question = row['clean_question'].split()\n",
    "    # word contains 6+ characters\n",
    "    split_question = [q for q in split_question if len(q)>= 6]\n",
    "    match_count = 0\n",
    "    for term in split_question:\n",
    "        if term in terms_used:\n",
    "            match_count += 1\n",
    "        terms_used.add(term)\n",
    "    if len(split_question) > 0:\n",
    "        # normalize the count across different question length\n",
    "        match_count = match_count / len(split_question)\n",
    "    question_overlap.append(match_count)\n",
    "jeopardy['question_overlap'] = question_overlap\n",
    "# get the percentage of the repeated question\n",
    "jeopardy['question_overlap'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 69% of the complex words in questions are repeated so it seems studying the past questions can be really helpful to win.\n",
    "\n",
    "## Study questions with high value\n",
    "\n",
    "Let's focus our study on questions that pertain to high value questions instead of low value questions. This is helpful to earn more money.\n",
    "\n",
    "We can actually figure out which terms correspond to high-value questions using a chi-squared test. I'll first need to narrow down the questions into two categories:\n",
    "\n",
    "* Low value -- Any row where Value is less than 800.\n",
    "* High value -- Any row where Value is greater than 800.\n",
    "\n",
    "I'll then be able to loop through each of the terms from `terms_used`, and:\n",
    "\n",
    "* Find the number of low value questions the word occurs in.\n",
    "* Find the number of high value questions the word occurs in.\n",
    "* Find the percentage of questions the word occurs in.\n",
    "* Based on the percentage of questions the word occurs in, find expected counts.\n",
    "* Compute the chi-squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "I can then find the words with the biggest differences in usage between high and low value questions, by selecting the words with the highest associated chi-squared values. Doing this for all of the words would take a very long time, so I'll just do it for a small sample now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_value(row):\n",
    "    value = 0\n",
    "    if row['clean_value'] > 800:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "jeopardy['high_value'] = jeopardy.apply(categorize_value, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for _, row in jeopardy.iterrows():\n",
    "        split_question = row['clean_question'].split()\n",
    "        if word in split_question:\n",
    "            if row['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recruits',\n",
       " 'hotshot',\n",
       " '500000member',\n",
       " 'exceptions',\n",
       " 'dipsomaniac',\n",
       " 'tylenol',\n",
       " 'letters',\n",
       " 'latvia',\n",
       " 'bergens',\n",
       " 'strangely']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly pick ten elements of terms_used\n",
    "from random import choice\n",
    "comparison_terms = [choice(list(terms_used)) for _ in range(10)]\n",
    "comparison_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (17, 37),\n",
       " (3, 1),\n",
       " (0, 1),\n",
       " (1, 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_expected = []\n",
    "for word in comparison_terms:\n",
    "    observed_expected.append(count_values(word))\n",
    "observed_expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the observed counts for a few terms, we can compute the expected counts and the chi-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_value_count = 5734\n",
      "low_value_count = 14265\n"
     ]
    }
   ],
   "source": [
    "high_value_count = sum(jeopardy['high_value'])\n",
    "low_value_count = jeopardy[jeopardy['high_value'] == 0]['high_value'].count()\n",
    "# low_value_count2 = jeopardy['high_value'].count() - sum(jeopardy['high_value'])\n",
    "\n",
    "print('high_value_count = {}'.format(high_value_count))\n",
    "print('low_value_count = {}'.format(low_value_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=0.4448774816612795, pvalue=0.5047776487545996),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.20850107809730017, pvalue=0.6479447887525934),\n",
       " Power_divergenceResult(statistic=4.198022975221989, pvalue=0.0404711362009595),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.03188116723440362, pvalue=0.8582887163235293)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "chi_squared = []\n",
    "for high_count, low_count in observed_expected:\n",
    "    # total times that a word shows\n",
    "    total = high_count + low_count\n",
    "    # the probability of a word shows in jeopardy\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    # expected values according to the ratio of total high/low values\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([high_count, low_count])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    \n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "    \n",
    "chi_squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above result none of the p values is less than 0.05 so there is no significant difference in usage in high value and low value for these words. Additionally, the frequencies were all except one lower than 5, so the chi-squared test isn't as valid. It would be better to run this test with only terms that have higher frequencies.\n",
    "\n",
    "## Eliminate non-informative words\n",
    "\n",
    "We can eliminate non-informative words to decrease the size of `terms_used` so we are able to run `count_values` function on more data. First we can remove `stopwords`.\n",
    "\n",
    "### Remove stopwords\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. We would not want these words to take up space in our database, or taking up the valuable processing time. Let's remove these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24470"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24454"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in stop_words:\n",
    "    if word in terms_used:\n",
    "        terms_used.remove(word)\n",
    "len(terms_used)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hrefhttp\n",
    "looking at the words in `terms_used` there are some links which seem not relevant to our project question, so we can remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23251"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_lr = pd.Series(list(terms_used))\n",
    "# The tilde (~) operator is used to invert the boolean values \n",
    "terms_used_lr = terms_used_lr[~terms_used_lr.str.contains('hrefhttp')]\n",
    "len(terms_used_lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still 23250 words in `terms_used`. At this stage, we can look at the `count_values` function and see if I can make it run faster.\n",
    "\n",
    "### Re-write count_values function\n",
    "\n",
    "Looking at the `count_values` function there is a loop that iterates over the whole jeopardy dataset. we can replace it with the pandas columns operations to make it faster. To make it easier to understand the result, the new function returns the word as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_faster(word):\n",
    "    high_count = 0\n",
    "    low_count = 0\n",
    "    \n",
    "    # regex pattern to match the whole word only\n",
    "    pattern = r\"\\b{}\\b\".format(word)\n",
    "    high_count = jeopardy[(jeopardy['clean_question'].str.contains(pattern, regex = True)) &\n",
    "                         (jeopardy['high_value'] == 1)]['high_value'].count()\n",
    "    low_count = jeopardy[(jeopardy['clean_question'].str.contains(pattern, regex = True)) &\n",
    "                        (jeopardy['high_value'] == 0)]['high_value'].count()\n",
    "    return word, high_count, low_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to make sure that we get the same result as the count_values function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('recruits', 1, 1), ('hotshot', 1, 0), ('500000member', 0, 1), ('exceptions', 0, 1), ('dipsomaniac', 0, 1), ('tylenol', 0, 1), ('letters', 17, 37), ('latvia', 3, 1), ('bergens', 0, 1), ('strangely', 1, 2)]\n"
     ]
    }
   ],
   "source": [
    "observed_test = []\n",
    "for word in comparison_terms:\n",
    "    observed_test.append(count_values_faster(word))\n",
    "print(observed_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is passed and the results are the same with higher efficiency.\n",
    "\n",
    "I am going to apply this new function on the all `terms_used`. It takes time to run completely but it is more applicable than count_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             (boasts, 5, 6)\n",
       "1          (integrity, 1, 0)\n",
       "2            (puberty, 0, 1)\n",
       "3            (gosling, 2, 0)\n",
       "4             (seward, 0, 2)\n",
       "                ...         \n",
       "24449      (beatified, 0, 1)\n",
       "24450         (boxers, 0, 1)\n",
       "24451    (modernqueen, 0, 1)\n",
       "24452     (arthropods, 1, 0)\n",
       "24453        (waldorf, 1, 1)\n",
       "Length: 23251, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = terms_used_lr.apply(count_values_faster)\n",
    "frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words with higher frequencies\n",
    "\n",
    "To make the `chi_squared` test valid, let's filter the words with high frequency and run the chio squred test on the top 1000 highest frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "      <th>total_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>called</td>\n",
       "      <td>168</td>\n",
       "      <td>346</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>country</td>\n",
       "      <td>141</td>\n",
       "      <td>332</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19362</th>\n",
       "      <td>played</td>\n",
       "      <td>77</td>\n",
       "      <td>212</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>became</td>\n",
       "      <td>79</td>\n",
       "      <td>203</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>american</td>\n",
       "      <td>77</td>\n",
       "      <td>174</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17472</th>\n",
       "      <td>controversial</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>consists</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>stopped</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16451</th>\n",
       "      <td>figures</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16267</th>\n",
       "      <td>waterfall</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  high_value  low_value  total_value\n",
       "867           called         168        346          514\n",
       "2728         country         141        332          473\n",
       "19362         played          77        212          289\n",
       "8683          became          79        203          282\n",
       "4831        american          77        174          251\n",
       "...              ...         ...        ...          ...\n",
       "17472  controversial           4         10           14\n",
       "5216        consists           3         11           14\n",
       "5286         stopped           1         13           14\n",
       "16451        figures           4         10           14\n",
       "16267      waterfall           5          9           14\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_high_frequecies(data, size):\n",
    "    frequencies = pd.DataFrame(data, \n",
    "                               columns = ['word', 'high_value', 'low_value'])\n",
    "    frequencies['total_value'] = frequencies['high_value'] + frequencies['low_value']\n",
    "    frequencies.sort_values('total_value', ascending = False, inplace = True)\n",
    "    return(frequencies.head(size))\n",
    "\n",
    "\n",
    "\n",
    "high_frequecies = get_high_frequecies(list(frequencies),1000)\n",
    "high_frequecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867      [(called, 4.048305063534577, 0.044215717944225...\n",
       "2728     [(country, 0.29967829483482744, 0.584084171311...\n",
       "19362    [(played, 0.5810990283039111, 0.44588185909193...\n",
       "8683     [(became, 0.05956570730840162, 0.8071836789959...\n",
       "4831     [(american, 0.4938111242657224, 0.482232156839...\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_chi_squared(row):\n",
    "    chi_squared = []\n",
    "    total_prop = row['total_value']/jeopardy.shape[0]\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    observed = np.array([row['high_value'], row['low_value']])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    \n",
    "    chi_value, p_value = chisquare(observed, expected)\n",
    "    \n",
    "    chi_squared.append((row['word'], chi_value, p_value, row['high_value'], row['low_value']))\n",
    "    return chi_squared\n",
    "                         \n",
    "    \n",
    "chi_squared = high_frequecies.apply(calculate_chi_squared, axis = 1)\n",
    "chi_squared.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we can filter the words with the `p_values` less than 0.05 to figure out which words are significantly different in high value and low value. I am also looking for words with higher frequency in `high_value` questions rather than `low_value` ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>chi_squared</th>\n",
       "      <th>p_value</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>monitora</td>\n",
       "      <td>45.947439</td>\n",
       "      <td>1.214686e-11</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>target_blanksarah</td>\n",
       "      <td>24.358972</td>\n",
       "      <td>7.995351e-07</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>target_blankkelly</td>\n",
       "      <td>20.921282</td>\n",
       "      <td>4.785483e-06</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>african</td>\n",
       "      <td>17.283572</td>\n",
       "      <td>3.219584e-05</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>painter</td>\n",
       "      <td>16.941684</td>\n",
       "      <td>3.854581e-05</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>target_blankjimmy</td>\n",
       "      <td>16.114608</td>\n",
       "      <td>5.962236e-05</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>target_blankjon</td>\n",
       "      <td>13.979777</td>\n",
       "      <td>1.847876e-04</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>pulitzer</td>\n",
       "      <td>13.429676</td>\n",
       "      <td>2.476749e-04</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>liquid</td>\n",
       "      <td>12.719123</td>\n",
       "      <td>3.619354e-04</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>example</td>\n",
       "      <td>11.997980</td>\n",
       "      <td>5.325823e-04</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>spirit</td>\n",
       "      <td>11.341071</td>\n",
       "      <td>7.581159e-04</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>andrew</td>\n",
       "      <td>11.049381</td>\n",
       "      <td>8.871680e-04</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>plants</td>\n",
       "      <td>9.954357</td>\n",
       "      <td>1.604691e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>relative</td>\n",
       "      <td>9.954357</td>\n",
       "      <td>1.604691e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>border</td>\n",
       "      <td>9.792563</td>\n",
       "      <td>1.752191e-03</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>process</td>\n",
       "      <td>9.542069</td>\n",
       "      <td>2.008152e-03</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>physics</td>\n",
       "      <td>8.682874</td>\n",
       "      <td>3.212141e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>spiritual</td>\n",
       "      <td>8.682874</td>\n",
       "      <td>3.212141e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>string</td>\n",
       "      <td>8.057304</td>\n",
       "      <td>4.532057e-03</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>elements</td>\n",
       "      <td>7.198788</td>\n",
       "      <td>7.295282e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>marine</td>\n",
       "      <td>6.779092</td>\n",
       "      <td>9.223181e-03</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>jersey</td>\n",
       "      <td>6.652781</td>\n",
       "      <td>9.900120e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>greece</td>\n",
       "      <td>6.361380</td>\n",
       "      <td>1.166308e-02</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>translated</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>window</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>filled</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>composed</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>persian</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>particles</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>colony</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>physicist</td>\n",
       "      <td>5.549240</td>\n",
       "      <td>1.848871e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>freedom</td>\n",
       "      <td>5.333590</td>\n",
       "      <td>2.091826e-02</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>committee</td>\n",
       "      <td>4.896281</td>\n",
       "      <td>2.691459e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>4.896281</td>\n",
       "      <td>2.691459e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>describe</td>\n",
       "      <td>4.460992</td>\n",
       "      <td>3.467736e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>nature</td>\n",
       "      <td>4.460992</td>\n",
       "      <td>3.467736e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  chi_squared       p_value  high_value  low_value\n",
       "179           monitora    45.947439  1.214686e-11          35         13\n",
       "78   target_blanksarah    24.358972  7.995351e-07          40         33\n",
       "226  target_blankkelly    20.921282  4.785483e-06          25         16\n",
       "93             african    17.283572  3.219584e-05          35         33\n",
       "494            painter    16.941684  3.854581e-05          16          8\n",
       "159  target_blankjimmy    16.114608  5.962236e-05          28         24\n",
       "217    target_blankjon    13.979777  1.847876e-04          23         19\n",
       "498           pulitzer    13.429676  2.476749e-04          15          9\n",
       "388             liquid    12.719123  3.619354e-04          17         12\n",
       "467            example    11.997980  5.325823e-04          15         10\n",
       "592             spirit    11.341071  7.581159e-04          13          8\n",
       "689             andrew    11.049381  8.871680e-04          12          7\n",
       "557             plants     9.954357  1.604691e-03          13          9\n",
       "547           relative     9.954357  1.604691e-03          13          9\n",
       "309             border     9.792563  1.752191e-03          18         16\n",
       "422            process     9.542069  2.008152e-03          15         12\n",
       "991            physics     8.682874  3.212141e-03           9          5\n",
       "947          spiritual     8.682874  3.212141e-03           9          5\n",
       "439             string     8.057304  4.532057e-03          14         12\n",
       "885           elements     7.198788  7.295282e-03           9          6\n",
       "625             marine     6.779092  9.223181e-03          11          9\n",
       "461             jersey     6.652781  9.900120e-03          13         12\n",
       "721             greece     6.361380  1.166308e-02          10          8\n",
       "829         translated     5.950459  1.471346e-02           9          7\n",
       "848             window     5.950459  1.471346e-02           9          7\n",
       "861             filled     5.950459  1.471346e-02           9          7\n",
       "590           composed     5.772340  1.628035e-02          11         10\n",
       "618            persian     5.772340  1.628035e-02          11         10\n",
       "584          particles     5.772340  1.628035e-02          11         10\n",
       "602             colony     5.772340  1.628035e-02          11         10\n",
       "992          physicist     5.549240  1.848871e-02           8          6\n",
       "694            freedom     5.333590  2.091826e-02          10          9\n",
       "788          committee     4.896281  2.691459e-02           9          8\n",
       "792         portuguese     4.896281  2.691459e-02           9          8\n",
       "919           describe     4.460992  3.467736e-02           8          7\n",
       "907             nature     4.460992  3.467736e-02           8          7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [c[0] for c in chi_squared]\n",
    "chi_squared_df = pd.DataFrame([c[0] for c in chi_squared], \n",
    "                              columns = ['word', 'chi_squared', 'p_value', 'high_value', 'low_value'])\n",
    "chi_squared_df = chi_squared_df.sort_values('p_value')\n",
    "chi_squared_df = chi_squared_df[(chi_squared_df['p_value'] < 0.05) & \n",
    "                                (chi_squared_df['high_value'] > chi_squared_df['low_value']) ]\n",
    "chi_squared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, a dataset of Jeopardy questions has been used to figure out some patterns in the questions that could help to win. After exploring we figured out that\n",
    "\n",
    "* On average about 6% of the words of answers are found in the questions. So the chance of deducing the answer from the question is quite low.\n",
    "* About 69% of the complex words in questions are repeated so studying the past questions can be really helpful to win.\n",
    "\n",
    "Then we focused our study on questions that pertain to high value questions instead of low value ones. This is helpful to earn more money. Using chi squared test we have got a list of 36 words with higher usage in high value questions and with a statistically significant difference of usage in high value and low value questions.\n",
    "\n",
    "The next step can be finding the questions with the high value containing these words. These questions can be recommended to study to win."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
